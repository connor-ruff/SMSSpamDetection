{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "747 of 5572 messages are spam: 13.406317300789663%\n",
      "  Classification                                               Text  Result  \\\n",
      "0            ham  Go until jurong point, crazy.. Available only ...       0   \n",
      "1            ham                      Ok lar... Joking wif u oni...       0   \n",
      "2           spam  Free entry in 2 a wkly comp to win FA Cup fina...       1   \n",
      "3            ham  U dun say so early hor... U c already then say...       0   \n",
      "4            ham  Nah I don't think he goes to usf, he lives aro...       0   \n",
      "\n",
      "   Message_Size                                             Tokens  \\\n",
      "0           111  [Go, until, jurong, point, ,, crazy, .., Avail...   \n",
      "1            29           [Ok, lar, ..., Joking, wif, u, oni, ...]   \n",
      "2           155  [Free, entry, in, 2, a, wkly, comp, to, win, F...   \n",
      "3            49  [U, dun, say, so, early, hor, ..., U, c, alrea...   \n",
      "4            61  [Nah, I, do, n't, think, he, goes, to, usf, ,,...   \n",
      "\n",
      "                                     Filtered_Tokens  \n",
      "0  [Go, jurong, point, ,, crazy, .., Available, b...  \n",
      "1           [Ok, lar, ..., Joking, wif, u, oni, ...]  \n",
      "2  [Free, entry, 2, wkly, comp, win, FA, Cup, fin...  \n",
      "3  [U, dun, say, early, hor, ..., U, c, already, ...  \n",
      "4  [Nah, I, n't, think, goes, usf, ,, lives, arou...  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def eliminateStop(words):\n",
    "    returnList = [w for w in words if not w in stop_words]\n",
    "    return returnList\n",
    "\n",
    "def main():\n",
    "    # Read In Data and Format It Better\n",
    "    df = pd.read_csv('spam.csv', sep=',', encoding='ISO-8859-1')\n",
    "    df = df.drop(columns=[ \"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\n",
    "\n",
    "    # Add 2 New Columns: For binary spam/ham indicator and length of message\n",
    "    df['Result']= df['Classification'].map( {'spam' : int(1), 'ham' : int(0)})\n",
    "    df['Message_Size'] = df['Text'].apply(len)\n",
    "\n",
    "\n",
    "    # Calculate Statistics\n",
    "    totalMessages = df['Result'].count()\n",
    "    numSpams = df[df['Result']==1]['Result'].count()\n",
    "    numValid = df[df['Result'] == 0]['Result'].count()\n",
    "\n",
    "    # Print Distribution\n",
    "    print(f'{numSpams} of {totalMessages} messages are spam: {((numSpams/totalMessages)*100)}%')\n",
    "   \n",
    "    # Tokenize Each Message and add the list of tokens to the DataFrame\n",
    "    df['Tokens'] = df['Text'].apply(word_tokenize)\n",
    "    \n",
    "    # Add Tokenize With Removed Stop Words\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    df['Filtered_Tokens'] = df['Tokens'].apply(eliminateStop)\n",
    "    \n",
    "    print(df.head())\n",
    "    \n",
    "if __name__ =='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
